
--bring up docker


docker compose up -d


-- Different URL's

Flink Dashboard -> http://localhost:8081

AKHQ Dashboard -> http://localhost:8085

REST Catalog -> http://localhost:8181/v1/namespaces (check namespaces)

Minio -> http://localhost:9001/browser/warehouse (admin/password)

Trino -> http://localhost:8080 (admin)

Superset -> http://localhost:8088 (admin/admin)



-- Launch Flink SQL shell

docker exec -it sql-client ./sql-client.sh


--Set properties in Flink SQL session


SET sql-client.execution.result-mode=tableau;

SET 'execution.checkpointing.interval' = '10sec';

SET execution.runtime-mode = streaming;

-- create Datagen connector to push page views messages

curl --location --request PUT 'http://localhost:8083/connectors/datagen_local_01/config' \
--header 'Content-Type: application/json' \
--data '{
            "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",
            "key.converter": "org.apache.kafka.connect.storage.StringConverter",
            "kafka.topic": "pageviews",
            "quickstart": "pageviews",
            "max.interval": 500,
            "iterations": 10000,
            "tasks.max": "1"
        }'


-- Create page_views table in Flink from the Kafka events in page views topic

CREATE TABLE page_views (
        viewtime BIGINT,
        userid STRING,
        pageid STRING
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'pageviews',
        'scan.startup.mode' = 'earliest-offset',
        'properties.bootstrap.servers' = 'broker:29092',
        'value.format' = 'avro-confluent',
        'value.avro-confluent.schema-registry.url' = 'http://schemaregistry:8084'
    );

-- Create users table in Flink from JSON message in file system

CREATE TABLE
users (
userid STRING,
regionid STRING

    ) WITH (
    'connector' = 'filesystem',
	'path' = 'file:/opt/flink/input/users',
	'format'='json',
	'json.fail-on-missing-field' = 'false',
	'json.map-null-key-mode' = 'DROP',
        'source.monitor-interval' = '100'
);
  
-- Create pages table in Flink from JSON message in file system

CREATE TABLE
pages (
pageid STRING,
pagedescription STRING

    ) WITH (
    'connector' = 'filesystem',
	'path' = 'file:/opt/flink/input/pages',
	'format'='json',
	'json.fail-on-missing-field' = 'false',
	'json.map-null-key-mode' = 'DROP',
        'source.monitor-interval' = '100'
);

--create an iceberg catalog 

  CREATE CATALOG ice_catalog WITH (
    'type'='iceberg',
    'catalog-impl'='org.apache.iceberg.rest.RESTCatalog',
    'io-impl'='org.apache.iceberg.aws.s3.S3FileIO',
    'uri'='http://iceberg-rest:8181',
    'warehouse'='s3://warehouse/',
    'property-version'='1',
    's3.endpoint'='http://minio:9000',	
    's3.path-style-access'='true',
    's3.access-key-id'='admin',
    's3.secret-access-key'='password'
  );


use catalog ice_catalog;

CREATE DATABASE ice_db
  WITH (
    'db'='ice_db'
  );

use ice_db;
  

--sample table 

 CREATE TABLE employee (
      id   BIGINT,
      name STRING,
      dept STRING,
      dob  date
  ) PARTITIONED BY (
      dob
  );


--inserts

insert into employee values(1, 'Steve', 'HR', TO_DATE('1990-01-01')), (2, 'Bob', 'IT', TO_DATE('1995-01-01'));


 
insert into employee values(3, 'Smith', 'IT', TO_DATE('1990-01-01')), (4, 'Jack', 'HR', TO_DATE('1996-01-01'));


use catalog default_catalog;

--iceberg raw tables


CREATE TABLE ice_catalog.ice_db.ice_page_views
    AS
    SELECT * FROM page_views;


-- create ice_users and ice_pages

CREATE TABLE ice_catalog.ice_db.ice_users
    AS
    SELECT * FROM users;


CREATE TABLE ice_catalog.ice_db.ice_pages
    AS
    SELECT * FROM pages;



--jojn table


CREATE TABLE ice_catalog.ice_db.ice_user_page_views(
userid string,
pageid string,
regionid string,
pagedescription string
);

--insert data by joining the 3 tables
insert into ice_catalog.ice_db.ice_user_page_views select u.userid, pv.pageid, u.regionid, p.pagedescription from users u join page_views pv on(u.userid = pv.userid) join pages p on(pv.pageid = p.pageid);

-- Query and check the results

Select * from ice_catalog.ice_db.ice_user_page_views;

--query in streaming mode 

SELECT * FROM ice_catalog.ice_db.ice_user_page_views /*+ OPTIONS('streaming'='true', 'monitor-interval'='1s') */;


--switch to ice_catalog and ice_db and check some metadata queries
use catalog ice_catalog;

use ice_db;

--view the list of snapshots

select * from ice_page_views$snapshots;

--all manifests

select * from ice_page_views$all_manifests;

--branches and tags

SELECT * FROM ice_page_views$refs;

--Time travel 
-- https://www.epochconverter.com

select * from ice_page_views /*+ OPTIONS('as-of-timestamp'='1723995039000') */;

select * from ice_page_views /*+ OPTIONS('as-of-snapshot'='1723956820000') */;


--Setup a Dashboard in Superset to visualise some results

--Supserset DB connection to Trino

trino://admin:@trino:8080/flink_demo


-- streaming joins from iceberg tables itself

CREATE TABLE ice_catalog.ice_db.ice_user_page_views_1(
userid string,
pageid string,
regionid string,
pagedescription string
);


insert into ice_catalog.ice_db.ice_user_page_views_1 select u.userid, pv.pageid, u.regionid, p.pagedescription from ice_catalog.ice_db.ice_users /*+  OPTIONS('streaming'='true', 'monitor-interval'='10s')  */ u 
join ice_catalog.ice_db.ice_page_views /*+  OPTIONS('streaming'='true', 'monitor-interval'='15s')  */ pv on(u.userid = pv.userid) 
join ice_catalog.ice_db.ice_pages /*+  OPTIONS('streaming'='true', 'monitor-interval'='15s')  */ p on(pv.pageid = p.pageid);
